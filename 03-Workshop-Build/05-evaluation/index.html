
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../04-ideation/">
      
      
        <link rel="next" href="../06-operationalization/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.36">
    
    
      
        <title>5️⃣ | Evaluate with AI - Contoso Chat Workshop</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.06209087.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="pink">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#5-evaluate-with-ai" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Contoso Chat Workshop" class="md-header__button md-logo" aria-label="Contoso Chat Workshop" data-md-component="logo">
      
  <img src="../../img/logo.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Contoso Chat Workshop
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              5️⃣ | Evaluate with AI
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="pink"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 2c-1.82 0-3.53.5-5 1.35C8 5.08 10 8.3 10 12s-2 6.92-5 8.65C6.47 21.5 8.18 22 10 22a10 10 0 0 0 10-10A10 10 0 0 0 10 2"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="amber" data-md-color-accent="cyan"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Contoso Chat Workshop" class="md-nav__button md-logo" aria-label="Contoso Chat Workshop" data-md-component="logo">
      
  <img src="../../img/logo.svg" alt="logo">

    </a>
    Contoso Chat Workshop
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Build a Retail Copilot Code-First on Azure AI
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    00 Before You Begin
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            00 Before You Begin
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../00-Before-You-Begin/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    0️⃣ | Pre-Requisites
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    01 Tour Guide Setup
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            01 Tour Guide Setup
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../01-Tour-Guide-Setup/01-setup/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1️⃣ | Getting Started: Instructor-Led Workshop
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../01-Tour-Guide-Setup/02-validate/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2️⃣ | Validate Setup and Provision
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    02 Self Guide Setup
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            02 Self Guide Setup
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../02-Self-Guide-Setup/01-setup/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1️⃣ | Getting Started (Self-Guided Workshop)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../02-Self-Guide-Setup/02-provision/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2️⃣ | Provision Infra
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    03 Workshop Build
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            03 Workshop Build
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03-infra/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3️⃣ | Explore App Infrastructure
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04-ideation/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4️⃣ | Ideate With Prompty
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    5️⃣ | Evaluate with AI
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    5️⃣ | Evaluate with AI
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#ai-assisted-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      AI-Assisted Evaluation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-1-understand-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      Step 1: Understand Metrics
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-2-understand-evaluators" class="md-nav__link">
    <span class="md-ellipsis">
      Step 2: Understand Evaluators
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Step 2: Understand Evaluators">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-viewrun-all-evaluators" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 View/Run all evaluators.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-view-coherence-prompty" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 View Coherence Prompty
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-run-coherence-prompty" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 Run Coherence Prompty
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-3-run-batch-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      Step 3: Run Batch Evaluation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Step 3: Run Batch Evaluation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-run-evaluation-notebook" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 Run Evaluation Notebook
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-watch-evaluation-runs" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 Watch Evaluation Runs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-explore-evaluation-trace" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 Explore: Evaluation Trace
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-4-understand-eval-workflow" class="md-nav__link">
    <span class="md-ellipsis">
      Step 4: Understand Eval Workflow
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Step 4: Understand Eval Workflow">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-explore-create-response" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 Explore: Create Response
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-explore-evaluate-response" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 Explore: Evaluate Response
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-explore-create-summary" class="md-nav__link">
    <span class="md-ellipsis">
      4.3 Explore: Create Summary
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44-understand-eval-results" class="md-nav__link">
    <span class="md-ellipsis">
      4.4 Understand: Eval Results
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-5-optional-homework" class="md-nav__link">
    <span class="md-ellipsis">
      Step 5 (Optional) Homework
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Step 5 (Optional) Homework">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-explore-observability" class="md-nav__link">
    <span class="md-ellipsis">
      5.1 Explore: Observability
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-explore-custom-evaluators" class="md-nav__link">
    <span class="md-ellipsis">
      5.2 Explore: Custom Evaluators
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06-operationalization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6️⃣ | Deploy with ACA
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    04 Workshop Wrapup
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            04 Workshop Wrapup
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../04-Workshop-Wrapup/07-cleanup/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    7️⃣ | Cleanup
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#ai-assisted-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      AI-Assisted Evaluation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-1-understand-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      Step 1: Understand Metrics
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-2-understand-evaluators" class="md-nav__link">
    <span class="md-ellipsis">
      Step 2: Understand Evaluators
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Step 2: Understand Evaluators">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-viewrun-all-evaluators" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 View/Run all evaluators.
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-view-coherence-prompty" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 View Coherence Prompty
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-run-coherence-prompty" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 Run Coherence Prompty
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-3-run-batch-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      Step 3: Run Batch Evaluation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Step 3: Run Batch Evaluation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-run-evaluation-notebook" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 Run Evaluation Notebook
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-watch-evaluation-runs" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 Watch Evaluation Runs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-explore-evaluation-trace" class="md-nav__link">
    <span class="md-ellipsis">
      3.3 Explore: Evaluation Trace
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-4-understand-eval-workflow" class="md-nav__link">
    <span class="md-ellipsis">
      Step 4: Understand Eval Workflow
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Step 4: Understand Eval Workflow">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-explore-create-response" class="md-nav__link">
    <span class="md-ellipsis">
      4.1 Explore: Create Response
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-explore-evaluate-response" class="md-nav__link">
    <span class="md-ellipsis">
      4.2 Explore: Evaluate Response
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-explore-create-summary" class="md-nav__link">
    <span class="md-ellipsis">
      4.3 Explore: Create Summary
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44-understand-eval-results" class="md-nav__link">
    <span class="md-ellipsis">
      4.4 Understand: Eval Results
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#step-5-optional-homework" class="md-nav__link">
    <span class="md-ellipsis">
      Step 5 (Optional) Homework
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Step 5 (Optional) Homework">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-explore-observability" class="md-nav__link">
    <span class="md-ellipsis">
      5.1 Explore: Observability
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-explore-custom-evaluators" class="md-nav__link">
    <span class="md-ellipsis">
      5.2 Explore: Custom Evaluators
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="5-evaluate-with-ai">5️⃣ | Evaluate with AI</h1>
<div class="admonition success">
<p class="admonition-title">Let's Review where we are right now</p>
<p><img alt="Dev Workflow" src="../../img/workshop-developer-flow.png" /></p>
<p>In the previous step, we learned to iteratively build our application prototype using Prompty assets and tooling, manually testing each iteration with a single test input.  In this <code>EVALUATE</code> stage, we now make test the application with a <strong>larger set of test inputs</strong>, using <strong>AI Assisted Evaluation</strong> to grade the responses (on a scale of <code>1-5</code>) for quality and safety based on pre-defined criteria.</p>
</div>
<h2 id="ai-assisted-evaluation">AI-Assisted Evaluation</h2>
<p>Evaluation helps us make sure our application meets desired quality and safety criteria in the responses it generates. In this section, we'll learn how to assess the <em>quality</em> of responses from our application using a 3-step workflow:</p>
<ol>
<li>We define a representative set of test inputs in a JSON file (see <code>evaluators/data.jsonl</code>)</li>
<li>Our application processes these inputs, storing the results (in <code>evaluators/results.jsonl</code>)</li>
<li>Our evaluators grade results for 4 quality metrics (in <code>evaluators/eval_results.jsonl</code>)</li>
</ol>
<p>While this workflow can be done manually, with a human grader providing the scores, it will not scale to the diverse test inputs and frequent design iterations required for generative AI applications. Instead, we use <strong>AI Assisted Evaluation</strong> effectively getting a second AI application (evaluator) to grade the first AI application (chat) - based on a scoring task that we define using a custom evaluator (Prompty). Let's see how this works.</p>
<h2 id="step-1-understand-metrics">Step 1: Understand Metrics</h2>
<p>The chat application generates its response (ANSWER) given a customer input (QUESTION) and support knowledge (CONTEXT) that can include the customer_id and chat_history. We then assess the <em>quality</em> of the ANSWER using 4 metrics, each scored on a scale of 1-5.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: left;">What it assesses</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>Coherence</strong></td>
<td style="text-align: left;">How well do all sentences in the ANSWER fit together? <br/> Do they sound natural when taken as a whole?</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Fluency</strong></td>
<td style="text-align: left;">What is the quality of individual sentences in the ANSWER? <br/> Are they well-written and grammatically correct?</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Groundedness</strong></td>
<td style="text-align: left;">Given support knowledge, does the ANSWER use the information provided by the CONTEXT?</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Relevance</strong></td>
<td style="text-align: left;">How well does the ANSWER address the main aspects o fthe QUESTION, based on the CONTEXT?</td>
</tr>
</tbody>
</table>
<h2 id="step-2-understand-evaluators">Step 2: Understand Evaluators</h2>
<p>The "scoring" task could be performed by a human, but this does not scale. Instead, we use AI-assisted evaluation by using one AI application ("evaluator") to grade the other ("chat"). And just like we used a <code>chat.prompty</code> to define our chat application, we can design <code>evaluator.prompty</code> instances that define the grading application - with a <strong>custom evaluator</strong> for each assessed metric.</p>
<h3 id="21-viewrun-all-evaluators">2.1 View/Run all evaluators.</h3>
<ol>
<li>Navigate to the <code>src/api/evaluators/custom_evals</code> folder in VS Code.</li>
<li>Open each of the 4 <code>.prompty</code> files located there, in the VS Code editor.<ul>
<li><code>fluency.prompty</code></li>
<li><code>coherence.prompty</code></li>
<li><code>groundedness.prompty</code></li>
<li><code>relevance.prompty</code></li>
</ul>
</li>
<li>Run each file and observe the output seen frm Prompty execution.</li>
<li><strong>Check:</strong> You see prompty for Coherence, Fluency, Relevance and Groundedness.</li>
<li><strong>Check:</strong> Running the prompty assets gives scores between <code>1</code> and <code>5</code></li>
</ol>
<p>Let's understand how this works, taking one of these custom evaluators as an example.</p>
<h3 id="22-view-coherence-prompty">2.2 View Coherence Prompty</h3>
<ol>
<li>
<p>Open the file <code>coherence.prompty</code> and look at its structure</p>
<ol>
<li>
<p>You should see: <strong>system</strong> task is</p>
<blockquote>
<p>You are an AI assistant. You will be given the definition of an evaluation metric for assessing the quality of an answer in a question-answering task. Your job is to compute an accurate evaluation score using the provided evaluation metric. You should return a single integer value between 1 to 5 representing the evaluation metric. You will include no other text or information.</p>
</blockquote>
</li>
<li>
<p>You should see: <strong>inputs</strong> expected are</p>
<ul>
<li><code>question</code> = user input to the chat model</li>
<li><code>answer</code> = response provided by the chat model</li>
<li><code>context</code> = support knowledge that the chat model was given</li>
</ul>
</li>
<li>
<p>You should see: <strong>meta-prompt</strong> guidance for the task:</p>
<blockquote>
<p>Coherence of an answer is measured by how well all the sentences fit together and sound naturally as a whole. Consider the overall quality of the answer when evaluating coherence. Given the question and answer, score the coherence of answer between one to five stars using the following rating scale:</p>
<ul>
<li>One star: the answer completely lacks coherence</li>
<li>Two stars: the answer mostly lacks coherence</li>
<li>Three stars: the answer is partially coherent</li>
<li>Four stars: the answer is mostly coherent</li>
<li>Five stars: the answer has perfect coherency</li>
</ul>
</blockquote>
</li>
<li>
<p>You should see: <strong>examples</strong> that provide guidance for the scoring.</p>
<blockquote>
<p>This rating value should always be an integer between 1 and 5. So the rating produced should be 1 or 2 or 3 or 4 or 5.
(See examples for question-answer-context inputs that reflect 1,2,3,4 and 5 scores)</p>
</blockquote>
</li>
</ol>
</li>
</ol>
<h3 id="23-run-coherence-prompty">2.3 Run Coherence Prompty</h3>
<ol>
<li>
<p>You see: <strong>sample input</strong> for testing</p>
<table>
<thead>
<tr>
<th style="text-align: left;">question</th>
<th style="text-align: left;">What feeds all the fixtures in low voltage tracks instead of each light having a line-to-low voltage transformer?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">answer</td>
<td style="text-align: left;">The main transformer is the object that feeds all the fixtures in low voltage tracks.</td>
</tr>
<tr>
<td style="text-align: left;">context</td>
<td style="text-align: left;">Track lighting, invented by Lightolier, was popular at one period of time because it was much easier to install than recessed lighting, and individual fixtures are decorative and can be easily aimed at a wall. It has regained some popularity recently in low-voltage tracks, which often look nothing like their predecessors because they do not have the safety issues that line-voltage systems have, and are therefore less bulky and more ornamental in themselves. A master transformer feeds all of the fixtures on the track or rod with 12 or 24 volts, instead of each light fixture having its own line-to-low voltage transformer. There are traditional spots and floods, as well as other small hanging fixtures. A modified version of this is cable lighting, where lights are hung from or clipped to bare metal cables under tension</td>
</tr>
</tbody>
</table>
</li>
<li>
<p>Run the prompty file. You see output like this. This means the evaluator "assessed" this ANSWER as being very coherent (score=5). </p>
<div class="highlight"><pre><span></span><code><span class="m">2024</span>-09-16<span class="w"> </span><span class="m">21</span>:35:43.602<span class="w"> </span><span class="o">[</span>info<span class="o">]</span><span class="w"> </span>Loading<span class="w"> </span>/workspaces/contoso-chat/.env
<span class="m">2024</span>-09-16<span class="w"> </span><span class="m">21</span>:35:43.678<span class="w"> </span><span class="o">[</span>info<span class="o">]</span><span class="w"> </span>Calling<span class="w"> </span>...
<span class="m">2024</span>-09-16<span class="w"> </span><span class="m">21</span>:35:44.488<span class="w"> </span><span class="o">[</span>info<span class="o">]</span><span class="w"> </span><span class="m">5</span>
</code></pre></div>
</li>
<li>
<p><strong>Observe:</strong> Recall that coherence is about how well the sentences fit together. </p>
<ul>
<li>Given the sample input, do you agree with the assessment?   </li>
</ul>
</li>
<li>
<p><strong>Change Answer</strong></p>
<ul>
<li>replace sample answer with: <code>Lorem ipsum orci dictumst aliquam diam</code> </li>
<li>run the prompty again. <em>How did the score change?</em></li>
<li>undo the change. Return the prompty to original state for the next step.</li>
</ul>
</li>
</ol>
<p>Repeat this exercise for the other evaluators on your own. Use this to build your intuition for each metric and how it defines and assesses response quality.</p>
<div class="admonition info">
<p class="admonition-title">Note the several examples given in the Prompty file of answers that represent each of the star ratings. This is an example of <a href="https://learn.microsoft.com/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions#few-shot-learning">few-shot learning</a>, a common technique used to guide AI models.</p>
</div>
<hr />
<h2 id="step-3-run-batch-evaluation">Step 3: Run Batch Evaluation</h2>
<p>In the previous section, we assessed a single answer for a single metric, running one Prompty at a time. In reality, we will need to run assessments automatically across a large set of test inputs, with all custom evaluators, before we can judge if the application is ready for production use. In this exercise, we'll run a batch evaluation on our Contoso Chat application, using a Jupyter notebook.</p>
<h3 id="31-run-evaluation-notebook">3.1 Run Evaluation Notebook</h3>
<p>Navigate to the <code>src/api</code> folder in Visual Studio Code.</p>
<ul>
<li>Click: <code>evaluate-chat-flow.ipynb</code> - see: A Jupyter notebook</li>
<li>Click: Select Kernel - choose "Python Environments" - pick recommended <code>Python 3.11.x</code></li>
<li>Click: <code>Run all</code> - this kickstarts the multi-step evaluation flow.</li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Troubleshooting: Evaluation gives an error message in the Notebook</p>
<p>On occasion, the evaluation notebook may throw an error after a couple of iterations. This is typically a transient error. To fix it, <code>Clear inputs</code> in the Jupyter Notebook, then <code>Restart</code> it. It should complete the run this time.</p>
</div>
<h3 id="32-watch-evaluation-runs">3.2 Watch Evaluation Runs</h3>
<p>One of the benefits of using Prompty is the built-in <code>Tracer</code> feature that captures execution traces for the entire workflow. These trace <em>runs</em> are stored in  <code>.tracy</code> files in in the <code>api/.runs/</code> folder as shown in the figure below.</p>
<ul>
<li>Keep this explorer sidebar open while the evaluation notebook runs/</li>
<li>You see: <code>get_response</code> traces when our chat application is running</li>
<li>You see: <code>groundedness</code> traces when its groundeness is evaluated</li>
<li>You see: similar <code>fluency</code>, <code>coherence</code> and <code>relevance</code> traces</li>
</ul>
<p><img alt="Eval" src="../../img/Evaluation%20Runs.png" /></p>
<h3 id="33-explore-evaluation-trace">3.3 Explore: Evaluation Trace</h3>
<p>Click on any of these <code>.tracy</code> files to launch the <em>Trace Viewer</em> window seen at right. </p>
<ul>
<li>Note that this may take a while to appear. </li>
<li>You may need to click several runs before seeing a full trace.</li>
</ul>
<p>Once the trace file is displayed, explore the panel to get an intuition for usage</p>
<ul>
<li>See: sequence of steps in orchestrated flow (left)</li>
<li>See: prompt files with <code>load-prepare-run</code> sub-traces</li>
<li>See: Azure OpenAIExecutor traces on model use</li>
<li>Click: any trace to see its timing and details in pane (right)</li>
</ul>
<div class="admonition info">
<p class="admonition-title">Want to learn more about Prompty Tracing? <a href="https://github.com/microsoft/prompty/tree/main/runtime/prompty#using-tracing-in-prompty">Explore the documentation</a> to learn how to configure your application for traces, and how to view and publish traces for debugging and observability.</p>
</div>
<h2 id="step-4-understand-eval-workflow">Step 4: Understand Eval Workflow</h2>
<div class="admonition note">
<p class="admonition-title">The evaluation flow takes 7-9 minutes to complete. Let's use the time to explore the code and understand the underlying workflow in more detail</p>
</div>
<h3 id="41-explore-create-response">4.1 Explore: Create Response</h3>
<ol>
<li>
<p>Open the file <code>src/api/evaluators/data.jsonl</code></p>
<ul>
<li>This file contains the suite of test questions, each associated with a specific customer.</li>
<li>Sample question: <em>"what is the waterproof rating of the tent I bought?"</em></li>
</ul>
</li>
<li>
<p>Take another look at  <code>src/api/evaluate-chat-flow.ipynb</code></p>
<ul>
<li>Look at Cell 3, beginning <code>def create_response_data(df):</code></li>
<li>For each question in the file, the <code>get_response</code> function (from our chat application) is invoked to generate the response and associated context</li>
<li>The {question, context, response} triples are then written to the <code>results.jsonl</code> file.</li>
</ul>
</li>
</ol>
<h3 id="42-explore-evaluate-response">4.2 Explore: Evaluate Response</h3>
<ol>
<li>Take another look at  <code>src/api/evaluate-chat-flow.ipynb</code><ul>
<li>Look a cell 4, beginning <code>def evaluate():</code></li>
<li><strong>Observe</strong>: It loads the results file from the previous step</li>
<li><strong>Observe</strong>: For each result in file, it extracts the "triple"</li>
<li><strong>Observe</strong>: For each triple, it executes the 4 evaluator Promptys</li>
<li><strong>Observe</strong>: It writes the scores to an <code>evaluated_results.jsonl</code> file</li>
</ul>
</li>
</ol>
<h3 id="43-explore-create-summary">4.3 Explore: Create Summary</h3>
<ol>
<li>
<p>When notebook execution completes, look in the <code>src/api/evaluators</code> folder:</p>
<ul>
<li>See: <strong>Chat Responses</strong> in <code>result.jsonl</code></li>
<li>See: <strong>Evaluated Results</strong> in <code>result_evaluated.jsonl</code> (scores at end of each line)</li>
<li>See: <strong>Evaluation Summary</strong> computed from <code>eval_results.jsonl</code> (complete data.)</li>
</ul>
</li>
<li>
<p>Scroll to the bottom of the notebook and view the results cell:</p>
<ul>
<li>Click the <code>View as scrollable element</code> link to redisplay output</li>
<li>Scroll to the bottom of redisplayed cell to view scores table</li>
<li>You should see something like this</li>
</ul>
</li>
</ol>
<p><img alt="Eval" src="../../img/tabular-eval.png" /></p>
<h3 id="44-understand-eval-results">4.4 Understand: Eval Results</h3>
<p>The figure shows you what that tabulated data looks like in the notebook results. Ignore the formatting for now, and let's look at what this tells us:</p>
<ol>
<li>You see 12 rows of data - correspoding to 12 test inputs</li>
<li>You see 3 columns of metrics - corresponding to evaluators</li>
<li>Each metric records a score between <code>1</code> and <code>5</code></li>
</ol>
<p>Let's try to put the scores in context of the responses we see. Try these exercises:</p>
<ol>
<li>Find a row that has a <code>groundedness</code> of 5.<ul>
<li>View the related row in the evaluation results file</li>
<li>Observe the answer and context provided - <em>was the answer grounded in the context?</em></li>
</ul>
</li>
<li>Find a row that has a <code>groundedness</code> of 1.<ul>
<li>View the related row in the evaluation results file</li>
<li>Observe the answer and context provided - <em>was THIS answer grounded in the context?</em></li>
</ul>
</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Explore the data in more detail on your own. Try to build your intuition for how scores are computed, and how that assessment reflects in the quality of your application.</p>
</div>
<h2 id="step-5-optional-homework">Step 5 (Optional) Homework</h2>
<div class="admonition success">
<p class="admonition-title">Congratulations! You just used custom evaluators in an AI-Assisted Evaluation flow!</p>
</div>
<p>We covered a lot in this section!! But there's a lot more left to learn. Here are two areas for you to explore on your own, when you revisit this workshop at home.</p>
<h3 id="51-explore-observability">5.1 Explore: Observability</h3>
<ul>
<li>Revisit the <code>contoso_chat/chat_request.py</code> and <code>evaluators/coherence.py</code> files<ul>
<li><strong>Observe:</strong> the <code>PromptyTracer</code> and <code>@trace</code> decoration features</li>
</ul>
</li>
<li>Look for the <code>src/api/.runs</code> folder and click on a <code>.tracy</code> file<ul>
<li><strong>Observe:</strong> the traces to understand the telemetry captured for debugging</li>
</ul>
</li>
<li>What happens when we remove a <code>@trace</code> annotation from a method?</li>
<li>What happens when we remove: <code>Tracer.add("PromptyTracer", json_tracer.tracer)</code></li>
</ul>
<h3 id="52-explore-custom-evaluators">5.2 Explore: Custom Evaluators</h3>
<ul>
<li>Copy the <code>Coherence.prompty</code> to a new <code>Politeness.prompty</code> file</li>
<li>Modify the <strong>system</strong> segment to define a "Politeness" metric</li>
<li>Modify the <strong>user</strong> segment to define your scoring guidanc</li>
<li>Define a sample input &amp; refine Prompty to return valid score</li>
<li>Create the test dataset, then assess results against your evaluator. </li>
<li>Think about how this approach extends to <em>safety</em> evaluations. </li>
</ul>
<hr />
<p><em>In this section, you saw how Prompty-based custom evaluators work with AI-Assisted evaluation, to assess the quality of your application using defined metrics like coherence, fluency, relevance, and groundedness. You got a sense for how these custom evaluators are crafted.</em></p>
<div class="admonition example">
<p class="admonition-title">Next → <a href="../06-operationalization/">Let's Talk About Deployment!</a> and related ideas for operationalization!</p>
</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["content.code.copy"], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.56dfad97.min.js"></script>
      
    
  </body>
</html>