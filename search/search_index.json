{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Let's Build Contoso Chat","text":"<p>About This Guide</p> <p>This developer guide teaches you how to build, run, evaluate, deploy, and use, an LLM application with Retrieval Augmented Generation. We'll walk you through the end-to-end development process from prompt engineering to LLM Ops with step-by-step instructions all the way. Here's what you need to know:</p> <ul> <li>We'll build Contoso Chat, a customer service AI application for the Contoso Outdoors company website. </li> <li>We'll use Azure AI Studio and Prompt Flow to streamline  LLMOps from ideation to operationalization!</li> </ul>"},{"location":"#what-youll-need","title":"What You'll Need","text":"<p>The main workshop takes about 60-75 minutes to complete. Significant time is taken by provisioning Azure resources and deploying your final promptflow-based LLM application. Some parts of the workshop may be automated or completed in parallel to reduce that time.</p> <p>You will need:</p> <ul> <li>A GitHub account (with GitHub Codespaces access)</li> <li>An Azure account (with Azure OpenAI access)</li> <li>A modern browser (to run Codespaces, access Azure portals)</li> <li>Familiarity with Python, Jupyter Notebooks &amp; VS Code</li> </ul> <p>Instructor Led Sessions</p> <p>This documentation is meant for self-guided completion of the workshop. The workshop may also be offered in instructor-led sessions at events like the 2024 Microsoft AI Tour using a Lab On Demand platform that comes with a pre-provisioned Azure subscription and built-in guide. Instructions should be comparable, but experience may vary.</p>"},{"location":"#what-youll-build","title":"What You'll Build","text":"<p>The main workshop focuses on building Contoso Chat, an AI application that uses Retrieval Augmented Generation to build a customer support chat agent for Contoso Outdoors, an online store for outdoor adventurers. The end goal is to integrate customer chat support into the website application for Contoso Outdoors, as shown below.</p> <p></p> <p>Azure-Samples Repositories</p> <p>The workshop will refer to two different applications in the overview. The contoso-chat sample provides the basis for building our RAG-based LLM Application to implement the chat-completion AI. The  contoso-web implements the Contoso Outdoors Web Application with an integrated chat interface that website visitors will use, to interact with our deployed AI application. We'll cover the details in the 01 | Introduction section of the guide.</p>"},{"location":"#how-youll-build-it","title":"How You'll Build It","text":"<p>The workshop is broadly organized into these steps, some of which may run in parallel.</p> <ul> <li> 1. Lab Overview</li> <li> 2. Setup Dev Environment (GitHub Codespaces)</li> <li> 3. Provision Azure (via Azure Portal, Azure AI Studio)</li> <li> 4. Configure VS Code (Azure Login, Populate Data)</li> <li> 5. Setup Promptflow (Local &amp; Cloud Connections)</li> <li> 6. Run &amp; Evaluate Flow (Locally on VS Code)</li> <li> 7. Deploy &amp; Use Flow (Cloud, via Azure AI Studio)</li> </ul> <p>With this workshop, you get hands-on experience with the ideating/exploring and building/augmenting phases of the LLM App development lifecycle. The workshop ends with deploying the endpoint to kickstart the operationalizing phase of LLM Ops.</p> <p></p> <p>If time permits, continue operationalizing your app with the following Bonus Exercises:</p> <ul> <li> 8.1 | Contoso Website Chat (Integration)</li> <li> 8.2 | GitHub Actions Deploy (Automation)</li> <li> 8.3 | Intent-based Routing (Multi-Agent)</li> <li> 8.4 | Content Filtering (Responsible AI)</li> </ul>"},{"location":"#get-started","title":"Get Started .. \ud83d\ude80","text":"<ul> <li>Want to understand the app dev lifecycle first? Start here \ud83d\udc49\ud83c\udffd 01 | Introduction.</li> <li>Want to jump straight into building the application? Start here \ud83d\udc49\ud83c\udffd  02 | Workshop.</li> <li>Want to learn more about LLM Ops? Watch this first \ud83d\udc47\ud83c\udffd #MSIgnite 2023 Breakout</li> </ul> <p>Breakout Session: End-to-End App Development: Prompt Engineering to LLM Ops</p> <p>Abstract | Prompt engineering and LLMOps are pivotal in maximizing the capabilities of Language Models (LLMs) for specific business needs. This session offers a comprehensive guide to Azure AI's latest features that simplify the AI application development cycle. We'll walk you through the entire process\u2014from prototyping and experimenting to evaluating and deploying your AI-powered apps. Learn how to streamline your AI workflows and harness the full potential of Generative AI with Azure AI Studio.</p> <p></p>"},{"location":"01%20%7C%20%20Introduction/1-paradigm-shift/","title":"01 | The Paradigm Shift","text":"<p>Streamlining the end-to-end development workflow for modern \"AI apps\" requires a paradigm shift from MLOps to LLMOps that acknowledges the common roots while being mindful of the growing differences. We can view this shift in terms of how it causes us to rethink three things (mindset, workflows, tools) to be more effective in developing generative AI applications.</p>"},{"location":"01%20%7C%20%20Introduction/1-paradigm-shift/#rethink-mindset","title":"Rethink Mindset","text":"<p>Traditional \"AI apps\" can be viewed as \"ML apps\". They took data inputs and used custom-trained models to return relevant predictions as output. Modern \"AI apps\" tend to refer to generative AI apps that take natural language inputs (prompts), use pre-trained large language models (LLM), and return original content to users as the response. This shift is reflected in many ways.</p> <ul> <li>The target audience is different \u27a1 App developers, not data scientists.</li> <li>The generated assets are different \u27a1 Emphasize integrations, not predictions.</li> <li>The evaluation metrics are different \u27a1 Focus on fairness, groundedness, token usage.</li> <li>The underlying ML models are different. Pre-trained \"Models-as-a-Service\" vs. build.</li> </ul> <p></p>"},{"location":"01%20%7C%20%20Introduction/1-paradigm-shift/#rethink-workflow","title":"Rethink Workflow","text":"<p>With MLOps, end-to-end application development involved a complex data science lifecycle. To \"fit\" into software development processes, this was mapped to a higher-level workflow visualized as shown below. The complex data science lifecycle steps (data preparation, model engineering &amp; model evaluation) are now encapsulated into the experimentation phase. </p> <p></p> <p>With LLMOps, those steps need to be rethought in the context of new requirements like using natural langauge inputs (prompts), new techniques for improving quality (RAG, Fine-Tuning), new metrics for evaluation (groundedness, coherence, fluency) and responsible AI (assessment). This leads us to a revised versio of the 3-phase application development lifecycle as shown:</p> <p></p> <p>We can unpack each phase to get a sense of individual steps in workflows that are now designed around prompt-based inputs, token-based pricing, and region-based availability of large language models and Azure AI services for provisioning.</p> <p></p> <p>Building LLM Apps: From Prompt Engineering to LLM Ops</p> <p>In the accompanying workshop, we'll walk through the end-to-end development process for our RAG-based LLM App from prompt engineering (ideation, augmentation) to LLM Ops (operationalization). We hope that helps make some of these abstract concepts feel more concrete when viewed in action.</p>"},{"location":"01%20%7C%20%20Introduction/1-paradigm-shift/#rethink-tools","title":"Rethink Tools","text":"<p>We can immediately see how this new application development lifecycle requires corresponding innovation in tooling to streamline the end-to-end development process from ideation to operationalization. The Azure AI platform has been retooled with exactly these requirements in mind. It is centered around Azure AI Studio, a unified web portal that:</p> <ul> <li>lets you \"Explore\" models, capabilities, samples &amp; responsible AI tools</li> <li>gives you single pane of glass visibility to \"Manage\" Azure AI resources</li> <li>provides UI-based development flows to \"Build\" your Azure AI projects</li> <li>has Azure AI SDK and Azure AI CLI options for \"Code-first\" development</li> </ul> <p></p> <p>The Azure AI platform is enhanced by other developer tools and resources including PromptFlow, Visual Studio Code Extensions and Responsible AI guidance with  built-in support for content-filtering. We'll cover some of these in the Concepts and Tooling sections of this guide.</p> <p>Using The Azure AI Platform</p> <p>The abstract workflow will feel more concrete when we apply the concepts to a real use case. In the Workshop section, you'll get hands-on experience with these tools to give you a sense of their roles in streamlining your end-to-end developer experience.</p> <ul> <li>Azure AI Studio: Build &amp; Manage Azure AI project and resources.</li> <li>Prompt Flow: Build, Evaluate &amp; Deploy a RAG-based LLM App.</li> <li>Visual Studio Code: Use Azure, PromptFlow, GitHub Copilot, Jupyter Notebook extensions.</li> <li>Responsible AI: Content filtering, guidance for responsible prompts usage.</li> </ul>"},{"location":"01%20%7C%20%20Introduction/2-scenario/","title":"02 | The App Scenario","text":"<p>Consider this familiar enterprise scenario!</p> <p>You're a new hire in the Contoso Outdoors organization. They are an e-commerce company with a successful product catalog and loyal customer base focused on outdoor activities like camping and hiking. Their website is hugely popular but their customer service agents are being overwhelmed by calls that could be answered by information currently on the site. </p> <p>For every call they fail to answer, they are potentially losing not just revenue, but customer loyalty. You are part of the developer team tasked to build a Customer Support AI into the website to meet that demand. The objective is to build and deploy a customer service agent that is friendly, helpful, responsible, and relevant in its support interactions.</p> <p>Let's walk through how you can make this happen in your organization, using the Azure AI Platform. Well start with the ideation phase which involves identifying the business case, connecting to your data, building the basic prompt flow (LLM App), then iterating locally to extend it for your app requirements. Let's understand how this maps to our workshop.</p>"},{"location":"01%20%7C%20%20Introduction/2-scenario/#contoso-outdoors-website","title":"Contoso Outdoors (Website)","text":"<p>The Contoso Chat (LLM App) is being designed for integration into the Contoso Outdoors site (Web App) via the chat icon seen at the bottom right. The website landing page features the Contoso Outdoors product catalog organized neatly into categories like Tents and Backpacks to simplify discovery by customers.</p> <p></p> <p>When a customer clicks an item, they are taken to the product details page with extensive information that they can use to guide their decisions towards a purchase.</p> <p></p> <p>Step 1: Identify Business Case</p> <p>The Contoso Chat AI should meet two business objectives. It should reduce customer support calls (to manual operator) by proactively answering customer questions onsite. It should increase customer product purchases by providing timely and contextual information to help them finalize the purchase decision. </p>"},{"location":"01%20%7C%20%20Introduction/2-scenario/#chat-completion-basic","title":"Chat Completion (Basic)","text":"<p>Let's move to the next step - designing our Contoso Chat AI using the relevant Large Language Model (LLM). Based on our manual customer service calls, we know questions can broadly fall into two categories:</p> <ul> <li>Product focus \u27a1 \"What should I buy for my hiking trip to Andalusia?\"</li> <li>Customer focus \u27a1 \"What backpack should I buy given my previous purchases here?\"</li> </ul> <p>We know that Azure OpenAI provides a number of pre-trained models for chat completion so let's see how the baseline model works for our requirements, by using the Azure AI Studio Playground capability with a <code>gpt-3.5-turbo</code> model deployment. This model can understand inputs (and generate responses) using natural language. </p> <p>Let's see how it responds to the two questions above.</p> <ul> <li> <p>1. No Product Context. The pre-trained model provides a perfectly valid response to the question but it lacks the product catalog context for Contoso Outdoors!. We need to refine this model to use our data!     </p> </li> <li> <p>2. No Customer History. The pre-trained model makes it clear that it has no access to customer history and consequently makes general recommendations that may be irrelevant to customer query. We need to refine this model to understand customer identity and access their purchase history.     </p> </li> </ul> <p>Step 2: Connect To Your Data</p> <p>We need a way to fine-tune the model to take our product catalog and customer history into account as relevant context for the query. The first step is to make the data sources available to our workflow. Azure AI Studio makes this easy by helping you setup and manage connections to relevant Azure search and database resources.</p>"},{"location":"01%20%7C%20%20Introduction/2-scenario/#chat-augmentation-rag","title":"Chat Augmentation (RAG)","text":"<p>That brings us to the next step - prompt engineering. We need to augment the user question (default prompt) with additional query context that ensures Contoso Outdoor product data is prioritized in responses. We use a popular technique know as Retrieval Augmented Generation (RAG) that works as shown below, to generate responses that are specific to your data. </p> <p></p> <p>We can now get a more grounded response in our Contoso Chat AI, as shown.</p> <p></p> <p>Step 3: Build Basic PromptFlow</p> <p>We now need to add in a step that also takes customer history into account. To implement this, we need a tool that helps us orchestrate these various steps in a more intuitive way, allowing user query and data to \"flow\" through the processing pipeline to generate the final response. </p>"},{"location":"01%20%7C%20%20Introduction/2-scenario/#chat-orchestration-flow","title":"Chat Orchestration (Flow)","text":"<p>The previous step gives us a basic flow that augments predefined model behaviors to add product context. Now, we want to add another tool (or processing function) that looks up customer details for additional prompt engineering. The end result should be a user experience that looks something like this, helping move the user closer to a purchase decision.</p> <p></p> <p>Step 4: Develop &amp; Extend Flow</p> <p>Flow orchestration is hard. This is where PromptFlow helps, allowing us to insert a customer lookup function seamlessly into the flow graph, to extend it. With the Azure AI platform, you get PromptFlow capabilities integrated seamlessly into both development (VS Code) and deployment (Azure AI Studio) environments for a streamlined end-to-end developer experience.</p>"},{"location":"01%20%7C%20%20Introduction/2-scenario/#evaluate-deploy-e2e","title":"Evaluate &amp; Deploy (E2E)","text":"<p>This ends the ideation phase of the application lifecycle we saw earlier. PromptFlow works seamlessly with Azure AI Studio to streamline the next two steps of the lifecycle (evaluate and deploy), helping get deliver the final Contoso Chat Support Agent AI experience on the Contoso Outdoors website. Here is what a multi-turn conversation with customers might look like now:</p> <p></p> <p>Your customer support AI is a hit! </p> <p>Not only can it answer questions grounded in your product catalog, but it can refine or recommend responses based on the customer's purchase history. The conversational experience feels more natural to your customers and reduces their effort in finding relevant products in information-dense websites.</p> <p>You find customers are spending more time in chat conversations with your support agent AI, and finding new reasons to purchase your products.</p> <p>Workshop: Build a production RAG with PromptFlow &amp; Azure AI Studio</p> <p>In the next section, we'll look at how we can bring this story to life, step-by-step, using Visual Studio Code, Azure AI Studio and Prompt Flow. You'll learn how to provision Azure AI Services, engineer prompts with Retrieval-Augmented Generation to use your product data, then extend the PromptFlow to include customer lookup before evaluating and deploying the Chat AI application to Azure for real-world use.</p>"},{"location":"01%20%7C%20%20Introduction/3-environment/","title":"03 | The Dev Environment","text":"<p>The repository is instrumented with dev container configuration that provides a consistent pre-built development environment deployed in a Docker container. Launch this in the cloud with GitHub Codespaces, or in your local device with Docker Desktop.</p>"},{"location":"01%20%7C%20%20Introduction/3-environment/#dev-tools","title":"Dev Tools","text":"<p>In addition, we make use of these tools:</p> <ul> <li>Visual Studio Code as the default editor | Works seamlessly with dev containers. Extensions streamline development with Azure and Prompt Flow. </li> <li>Azure Portal for Azure subscription management | Single pane of glass view into all Azure resources, activities, billing and more.</li> <li>Azure AI Studio (Preview) | Single pane of glass view into all resources and assets for your Azure AI projects. Currently in preview (expect it to evolve rapidly).</li> <li>Azure ML Studio | Enterprise-grade AI service for managing end-to-end ML lifecycle for operationalizing AI models. Used for some configuration operations in our workshop (expect support to move to Azure AI Studio).</li> <li>Prompt Flow | Open-source tooling for orchestrating end-to-end development workflow (design, implementation, execution, evaluation, deployment) for modern LLM applications.</li> </ul>"},{"location":"01%20%7C%20%20Introduction/3-environment/#required-resources","title":"Required Resources","text":"<p>We make use of the following resources in this lab:</p> <p>Azure Samples Used | Give them a \u2b50\ufe0f on GitHub</p> <ul> <li>Contoso Chat - as the RAG-based AI app we will build.</li> <li>Contoso Outdoors - as the web-based app using our AI.</li> </ul> <p>Azure Resources Used | Check out the Documentation</p> <ul> <li>Azure AI Resource - Top-level Azure resource for AI Studio, establishes working environment.</li> <li>Azure AI Project - saves state and organizes work for AI app development.</li> <li>Azure AI Search - get secure information retrieval at scale over user-owned content </li> <li>Azure Open AI - provides REST API access to OpenAI's powerful language models.</li> <li>Azure Cosmos DB - Fully managed, distributed NoSQL &amp; relational database for modern app development.</li> <li>Deployment Models Deployment from model catalog by various criteria.</li> </ul>"},{"location":"02%20%7C%20Workshop/01%20%7C%20Lab%20Overview/","title":"1.1 | What You'll Learn","text":"<p>This is a 60-75 minute workshop that consists of a series of lab exercises that teach you how to build a production RAG (Retrieval Augmented Generation) based LLM application using Promptflow and Azure AI Studio.</p> <p>You'll gain hands-on experience with the various steps involved in the end-to-end application development lifecycle from prompt engineering to LLM Ops.</p>"},{"location":"02%20%7C%20Workshop/01%20%7C%20Lab%20Overview/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this lab, you should be able to:</p> <ol> <li>Explain LLMOps - concepts &amp; differentiation from MLOps.</li> <li>Explain Prompt Flow - concepts &amp; tools for building LLM Apps.</li> <li>Explain Azure AI Studio - features &amp; functionality for streamlining E2E app development.</li> <li>Design, run &amp; evaluate RAG apps - using the Promptflow Extension on VS Code</li> <li>Deploy, test &amp; use RAG apps - from Azure AI Studio UI (no code experience)</li> </ol>"},{"location":"02%20%7C%20Workshop/01%20%7C%20Lab%20Overview/#pre-requisites","title":"Pre-Requisites","text":"<p>We assume you have familiarity with the following:</p> <ol> <li>Machine Learning &amp; Generative AI concepts</li> <li>Python &amp; Jupyter Notebook programming</li> <li>Azure, GitHub &amp; Visual Studio Code tooling</li> </ol> <p>You will need the following to complete the lab:</p> <ol> <li>Your own laptop (charged) with a modern browser</li> <li>A GitHub account with GitHub Codespaces quota.</li> <li>An Azure subscription with Azure OpenAI access.</li> <li>An Azure AI Search resource with Semantic Ranker enabled.</li> </ol>"},{"location":"02%20%7C%20Workshop/01%20%7C%20Lab%20Overview/#dev-environment","title":"Dev Environment","text":"<p>You'll make use of the following resources in this workshop:</p> <p>Code Samples (GitHub Repositories)</p> <ul> <li>Contoso Chat - source code for the RAG-based LLM app.</li> <li>Contoso Web - source code for the Next.js-based Web app.</li> </ul> <p>Developer Tools (local and cloud)</p> <ul> <li>Visual Studio Code - as the default editor</li> <li>Github Codespaces - as the dev container</li> <li>Azure AI Studio (Preview) - for AI projects</li> <li>Azure ML Studio - for minor configuration</li> <li>Azure Portal - for managing Azure resources</li> <li>Prompt Flow - for streamlining end-to-end LLM app dev</li> </ul> <p>Azure Resources (Provisioned in Subscription)</p> <ul> <li>Azure AI Resource - top-level AI resource, provides hosting environment for apps</li> <li>Azure AI Project - organize work &amp; save state for AI apps.</li> <li>Azure AI Search - full-text search, indexing &amp; information retrieval. (product data)</li> <li>Azure OpenAI Service - chat completion &amp; text embedding models. (chat UI, RAG)</li> <li>Azure Cosmos DB - globally-distributed multi-model database. (customer data)</li> <li>Azure Static Web Apps - optional, deploy Contoso Web application. (chat integration)</li> </ul> <p>===</p>"},{"location":"02%20%7C%20Workshop/02%20%7C%20Setup%20Dev%20Environment/01-get-started/","title":"2.1 | Get Started","text":"<p>These instructions are for self-guided learners only.</p> <p>You must have an active Azure subscription with access to the Azure OpenAI services used in this lab. You should also have your own laptop with a charger (or sufficient battery to power a 75-minute session). Complete these steps to get started:</p> <ul> <li> 01 | Launch the Edge Browser (Full-Screen)</li> <li> 02 | Navigate to https://portal.azure.com</li> <li> 03 | Login with your Azure credentials</li> <li> 04 | Click on Resource Groups option under Navigate</li> <li> 05 | Leave this browser tab open to this Azure Portal page.</li> </ul> <p>You can now move to the 2.2 | Launch Codespaces step.</p> <p>Congratulations! You're authenticated on Azure. Move to step 02 | Launch Codespaces.</p> <p>Click here for a timestamped video walkthrough of a Skillable session, for quick reference.</p> <p> </p> <p>These instructions are for Skillable platform learners only.</p> <p>You will be given a link to launch the Skillable Lab. When launched, it will show you a screen (see below) with a built-in instruction manual on the right and a login screen on the left. Read the tips below, then switch over to that manual and continue following instructions there.</p> <p></p> <p>The Skillable Platform comes with:</p> <ul> <li> a built-in instruction manual. See screen at right for inline instructions.</li> <li> a pre-assigned Azure subscription. Look for the (username, password) details.</li> <li> handy one-click text entry cues. See green elements with \"T\" prefix.</li> </ul> <p>Here are a few tips to keep in mind:</p> <ul> <li>The text entry cues in the manual will auto-enter the associated text into the screen at left, at the current cursor location. Use this to reduce manual effort and ensure you use consistent and correct values for the many variables and literals in the lab.</li> <li>The Azure subscription provided is recycled at the end of each lab. You don't need to worry about deleting resources or taking any cleanup actions on Azure. That will be done for you.</li> <li>You will still use your personal GitHub login to launch GitHub Codespaces for the session. The lab can be completed within the generous \"free quota\" provided by GitHub Codespaces for personal accounts. If you have already used by that quota, you will either need a paid account - or need to setup a new account to get a fresh quota allocation.</li> <li>To prevent unnecessary depletion of that free quota, please remember to delete the Codespaces session at the end of this workshop.</li> </ul>"},{"location":"02%20%7C%20Workshop/02%20%7C%20Setup%20Dev%20Environment/02-launch-codespaces/","title":"2.2 | Launch Codespaces","text":"<p>These instructions are for self-guided learners only.</p> <p>Instructor-led sessions will use the built-in Skillable manual. However, the outcomes for each step will be the same, so we will reproduce screenshots (and link to video walkthroughs) from a Skillable session for a convenient reference.</p> <p>We'll use GitHub Codespaces as our development environment.</p> <p>The contoso-chat application sample comes with a devcontainer.json configuration file that provides a pre-built development environment with minimal manual effort required in setup. Let's get that running.</p> <ul> <li> 01 | Log into GitHub with your personal account.</li> <li> 02 | Navigate to this repo: Azure-Samples/contoso-chat</li> <li> 03 | Fork the repo into your GitHub profile</li> <li> 04 | Click \"Code\" dropdown, select \"Codespaces\" tab</li> <li> 05 | Click \"+\" to create new codespace</li> <li> 06 | Verify you see 'Setting up your codespace' in new tab</li> <li> 07 | Click \"View Logs\" to track progress</li> </ul> <p></p> <p>This step takes a few minutes to complete. It is configuring a Docker container (with a defined base image), installing the dependencies we've specified, and launching it with a built-in Visual Studio Code editor that is configured with required extensions.</p> <p>While we wait for setup to complete, let's move to the next step: 3 | Provision Azure</p> <p>Congratulations! Your development environment is being setup for you...</p> <p>Click here for a video walkthrough of this step in a Skillable session, for reference.</p>"},{"location":"02%20%7C%20Workshop/03%20%7C%20Provision%20Azure/03-create-airesource/","title":"3.1 | Azure AI Resource","text":"<p>These instructions are for self-guided learners only.</p> <p>In this section, we will manually provision all the necessary Azure resources for this lab. We may later document a scripted way to auto-provision them for efficiency. However, for now, we think manual setup will give you better insights into purpose &amp; configuration of each resource, and contribute to your learning journey.</p> <p>The Azure AI resource provides the hosting environment for your Azure AI application.</p> <p>Let's provision this resource manually.</p> <ul> <li> 01 | Navigate to https://ai.azure.com in a new tab.</li> <li> 02 | Click login. It should auto-login using prior Azure authentication.</li> <li> 03 | Click Manage in navbar.</li> <li> 04 | Click \"+ New Azure AI resource\" in page.</li> <li> 05 | Complete the pop-up dialog with these details:<ul> <li>Resource name: contoso-chat-ai</li> <li>Azure subscription: (leave default)</li> <li>Click \"Create new resource group\"<ul> <li>Resource group: contoso-chat-rg</li> <li>Location: Sweden Central</li> </ul> </li> </ul> </li> <li> 06 | Click \"Next: in pop-up dialog<ul> <li>Click Create to confirm resource creation.</li> <li>This takes a few minutes (see below). Wait for completion. </li> </ul> </li> <li> 07 | Return to \"Manage\" page &amp; Refresh.<ul> <li>Verify this Azure AI resource is listed. </li> </ul> </li> </ul> <p></p> <p>Congratulations! Your Azure AI Resource was created successfully.</p> <p>Click here for a video walkthrough of this step in a Skillable session, for reference.</p>"},{"location":"02%20%7C%20Workshop/03%20%7C%20Provision%20Azure/04-create-aiproject/","title":"04. Create Azure AI Project","text":"<p>[!NOTE] Continue here and create resource manually only if your Azure subscription was not pre-provisioned with a lab Resource Group</p> <ul> <li>[]  01 | Navigate to +++https://ai.azure.com+++ <ul> <li>Click the \"Build\" option in navbar.</li> <li>Click \"+ New project\"  in Projects table.</li> <li>You should see a dialog pop up.</li> </ul> </li> <li>[] 02 | Complete \"Project details\" dialog<ul> <li>Project name: +++contoso-chat-aiproj+++</li> <li>Azure AI resource: select resource you created.</li> <li>Click \"Create a project\"</li> <li>This will take a few minutes to complete</li> <li>You should be taken to created AI Project page</li> </ul> </li> <li>[] 03 | Go back to \"Build\" home page<ul> <li>Click Refresh. (may take a few secs to appear)</li> <li>Verify your Azure AI Project resource is listed. </li> </ul> </li> </ul> <p>\ud83e\udd73 Congratulations!  You're Azure AI Project resource is ready.</p>"},{"location":"02%20%7C%20Workshop/03%20%7C%20Provision%20Azure/05-model-deploy/","title":"05. Create Model Deployments","text":"<p>[!NOTE] Continue here and create resource manually only if your Azure subscription was not pre-provisioned with a lab Resource Group</p> <ul> <li> <p>[]  01 | Switch browser tab to +++https://ai.azure.com+++ </p> <ul> <li>Click the \"Build\" option in navbar.</li> <li>Click your AI Project to view its details page.</li> </ul> </li> <li> <p>[]  02 | Create gpt-3.5-turbo deployment</p> <ul> <li>Click the \"Deployments\" option in sidebar.</li> <li>Click +Create. Search for +++gpt-35-turbo+++ </li> <li>Select &amp; click \"Confirm\".</li> <li>Deployment Name: +++gpt-35-turbo+++</li> <li>Model version: select 0613 (for quota)</li> <li>Click Deploy. Should be fairly quick.</li> </ul> </li> <li> <p>[]  03 | Create gpt-4 deployment</p> <ul> <li>Click the \"Deployments\" option in sidebar.</li> <li>Click +Create. Search for +++gpt-4+++</li> <li>Select &amp; click \"Confirm\".</li> <li>Deployment Name: +++gpt-4+++</li> <li>Model version: leave it as default.</li> <li>Click Deploy. Should be fairly quick.</li> </ul> </li> <li> <p>[]  04 | Create text-embedding-ada-002 deployment</p> <ul> <li>Click the \"Deployments\" option in sidebar.</li> <li>Click +Create . Search for +++text-embedding-ada-002+++</li> <li>Select &amp; click \"Confirm\".</li> <li>Deployment Name: +++text-embedding-ada-002+++</li> <li>Model version: leave it as default.</li> <li>Click Deploy. Should be fairly quick.</li> </ul> </li> <li> <p>[] 05 | Return to \"Deployments\" home page </p> <ul> <li>Verify all 3 models are listed correctly</li> </ul> </li> </ul> <p>\ud83e\udd73 Congratulations!  You're Model Deployments are ready.</p>"},{"location":"02%20%7C%20Workshop/03%20%7C%20Provision%20Azure/06-create-aisearch/","title":"06. Create AI Search Resource","text":"<p>[!NOTE] Continue here and create resource manually only if your Azure subscription was not pre-provisioned with a lab Resource Group</p> <p>This step is done on the Azure Portal, not Azure AI Studio.</p> <ul> <li>[]  01 | Switch browser tab to +++https://portal.azure.com+++ <ul> <li>Click on \"Create a resource\" on the home page</li> <li>Search for +++Azure AI Search+++ in Create hub page.</li> <li>Click the Create drop down in the matching result.</li> </ul> </li> <li>[]  02 | Complete the \"Create a search service\" flow<ul> <li>Subscription - leave default</li> <li>Resource Group - select the resource group for AI project</li> <li>Service Name - use +++contoso-chat-aisearch+++</li> <li>Location - use +++East US+++ (Note the changed region!!)</li> <li>Pricing tier - check that it is set to Standard</li> </ul> </li> <li>[] 03 | Review and Create resource<ul> <li>Click \"Review and Create\" for a last review</li> <li>Click \"Create\" to confirm creation</li> <li>You should see a \"Deployment in progress\" indicator</li> </ul> </li> <li>[] 04 | Activate Semantic Search capability<ul> <li>Wait for the deployment to complete'</li> <li>Click Go to resource to visit AI Search resource page</li> <li>Click the Semantic Ranker option in the sidebar (left)</li> <li>Check Select Plan under Standard, to enable capability.</li> <li>Click Yes on the popup regarding service costs. </li> </ul> </li> </ul>"},{"location":"02%20%7C%20Workshop/03%20%7C%20Provision%20Azure/07-create-cosmosdb/","title":"07. Create CosmosDB Resource","text":"<p>[!NOTE] Continue here and create resource manually only if your Azure subscription was not pre-provisioned with a lab Resource Group</p> <p>We'll create this on Azure Portal, not Azure AI Studio.</p> <ul> <li>[]  01 | Switch browser tab to +++https://portal.azure.com+++ <ul> <li>Click on \"Create a resource\" on the home page</li> <li>Search for +++Azure CosmosDB+++ in Create hub page.</li> <li>Click the Create drop down in the matching result.</li> <li>You should see \"Which API best suits your workload?\"</li> </ul> </li> <li>[]  02 | Complete the Azure CosmosDB for NoSQL flow<ul> <li>Click Create on the \"Azure CosmosDB for NoSQL\" option</li> <li>Subscription - leave default</li> <li>Resource Group - select the resource group for AI project</li> <li>Account Name - use +++contoso-chat-cosmosdb+++</li> <li>Location - use +++Sweden Central+++ (same as others)</li> <li>Leave other options at defaults.</li> </ul> </li> </ul> <p>[!hint] If you get an error indicating the Account Name already exists, just add a number to make it unique - e.g., contoso-chat-cosmosdb-1</p> <ul> <li>[] 03 | Review and Create resource<ul> <li>Click \"Review and Create\" for a last review</li> <li>Click \"Create\" to confirm creation</li> <li>You should see a \"Deployment in progress\" indicator</li> </ul> </li> </ul> <p>Deployment completes in minutes. Visit resource to verify creation.</p> <p>\ud83e\udd73 Congratulations!  You're Azure CosmosDB Resource is ready.</p>"},{"location":"02%20%7C%20Workshop/04%20%7C%20Configure%20VS%20Code/08-az-login/","title":"08. VSCode Azure Login","text":"<p>[!NOTE] This assumes you did the 02. Launch GitHub Codespaces step previously and left that tab open for dev container setup to complete.</p> <ul> <li>[]  01 | Switch browser tab to your Visual Code editor session<ul> <li>You should see a Visual Studio Code editor </li> <li>You should see a terminal open in editor</li> </ul> </li> </ul> <p>[!hint] If VS Code Terminal is not open by default, click hamburger menu (top left), look for Terminal option &amp; Open New Terminal.</p> <ul> <li>[]  02 | Use Azure CLI from terminal, to login<ul> <li>Enter command: +++az login --use-device-code+++ </li> <li>Open +++https://microsoft.com/devicelogin+++ in new tab</li> <li>Copy-paste code from Azure CLI into the dialog you see here</li> <li>On success, close this tab and return to VS Code tab</li> </ul> </li> </ul> <p>\ud83e\udd73 Congratulations!  You're logged into Azure on VS Code.</p>"},{"location":"02%20%7C%20Workshop/04%20%7C%20Configure%20VS%20Code/09-az-config/","title":"09. VSCode Config Azure","text":"<p>[!NOTE] This assumes you have the Azure Resource Group and related resources provisioned correctly from prior steps. We'll now configure Visual Studio Code to use our provisioned Azure resources.</p> <ul> <li> <p>[]  01 | Download the 'config.json' for this Azure AI project</p> <ul> <li>Visit +++https://portal.azure.com+++ in a new browser tab</li> <li>Click on your created resource group (contoso-chat-rg)</li> <li>Click on your Azure AI project resource (contoso-chat-aiproj) </li> <li>Look for the download config.json option under Overview</li> <li>Click to download the file to the Windows 11 VM </li> <li>Open the file and Copy the contents to clipboard.</li> </ul> </li> <li> <p>[]  02 | Update your VS Code project with these values</p> <ul> <li>Switch browser tab to your Visual Studio Code editor</li> <li>Open VS Code Terminal, enter: +++touch config.json+++</li> <li>This creates an empty config.json file in root directory.</li> <li>Open file in VS Code and Paste data from clipboard</li> <li>Save the file.</li> </ul> </li> </ul>"},{"location":"02%20%7C%20Workshop/04%20%7C%20Configure%20VS%20Code/10-env-config/","title":"10. VSCode Config Env","text":"<p>[!hint] This assumes you the Azure Resource Group and related resources were setup previously. We'll now configure service endpoints and keys as env vars for programmatic access from Jupyter Notebooks.</p> <ul> <li>[]  01 | Keep your Visual Studio Code editor open in one tab<ul> <li>Find the local.env file in the root directory</li> <li>Open VS Code Terminal, enter: +++cp local.env .env+++</li> <li>This should copy \"local.env\" to a new .env file.</li> <li>Open \".env\" in Visual Studio Code, keep tab open.</li> </ul> </li> </ul> <p>[!hint]  This involves multiple Copy-Paste actions. If you have trouble pasting into VS Code window, right-click and choose Paste from the menu.</p> <ul> <li> <p>[]  02 | Update the Azure OpenAI environment variables</p> <ul> <li>Open +++https://ai.azure.com+++ in a new tab</li> <li>Click \"Build\", then open your AI project page.</li> <li>Click \"Settings\", click \"Show endpoints\" in the first tile</li> <li>Copy Azure.OpenAI endpoint value,  To \"CONTOSO_AI_SERVICES_ENDPOINT\" value in \".env\"</li> <li>Copy Primary key value  To \"CONTOSO_AI_SERVICES_KEY\" value in \".env\"</li> </ul> </li> <li> <p>[]  03 | Update the Azure AI Search environment variables</p> <ul> <li>Open +++https://portal.azure.com+++ in a new tab</li> <li>Open your Azure AI Search resource page (contoso-chat-aisearch)</li> <li>Copy Uri value under Overview page  To \"CONTOSO_SEARCH_SERVICE\" in \".env\"</li> <li>Copy Primary admin key value under Keys page  To \"CONTOSO_SEARCH_KEY\" in \".env\"</li> </ul> </li> <li> <p>[]  04 | Locate the Azure CosmosDB environment variables</p> <ul> <li>Open +++https://portal.azure.com+++ in a new tab</li> <li>Open your Azure CosmosDB resource page, click Keys</li> <li>Copy URI value,  To \"COSMOS_ENDPOINT\" value in \".env\"</li> <li>Copy PRIMARY KEY value  To \"COSMOS_KEY\" value in \".env\"</li> </ul> </li> <li> <p>[]  05 | Save the \".env\" file.</p> </li> </ul> <p>\ud83e\udd73 Congratulations!  Your VS Code env variables are updated!</p>"},{"location":"02%20%7C%20Workshop/05%20%7C%20Setup%20Promptflow/11-populate-index/","title":"11. VSCode Populate Search","text":"<p>[!NOTE] This assumes you setup the Azure AI Search resource earlier. In this section, we'll populate it with product data and create the index.</p> <ul> <li> <p>[]  01 | Return to the Visual Studio Code editor tab</p> <ul> <li>Locate the \"data/product_info/\" folder</li> <li>Open the create-azure-search.ipynb Jupyter Notebook.</li> </ul> </li> <li> <p>[]  02 | Run the notebook to populate search index</p> <ul> <li>Click Select Kernel (top right)</li> <li>Pick \"Python Environments\" and select recommended option</li> <li>Click Clear All Outputs then Run All</li> <li>Verify that all code cells executed correctly.</li> </ul> </li> <li> <p>[]  03 | Verify the search index was created</p> <ul> <li>Open +++https://portal.azure.com+++ to Azure AI Search resource</li> <li>Click the Indexes option in sidebar to view indexes</li> <li>Verify that the contoso-products search index was created.</li> </ul> </li> </ul> <p>\ud83e\udd73 Congratulations!  Your Azure AI Search index is ready!</p>"},{"location":"02%20%7C%20Workshop/05%20%7C%20Setup%20Promptflow/12-populate-db/","title":"12. VSCode Populate Database","text":"<p>[!NOTE] This assumes you setup the Azure CosmosDB resource earlier. In this section, we'll populate it with customer data.</p> <ul> <li> <p>[]  01 | Return to the Visual Studio Code editor tab</p> <ul> <li>Locate the \"data/customer_info/\" folder</li> <li>Open the create-cosmos-db.ipynb Jupyter Notebook.</li> </ul> </li> <li> <p>[]  02 | Run the notebook to populate customer database</p> <ul> <li>Click Select Kernel, set recommended Python environment </li> <li>Click Clear All Outputs then Run All &amp; verify completion</li> </ul> </li> <li> <p>[]  03 | Verify the customer database was created</p> <ul> <li>Open +++https://portal.azure.com+++ to Azure CosmosDB resource</li> <li>Click the Data Explorer option in sidebar to view data</li> <li>Verify that the contoso-outdoor container was created</li> <li>Verify that it contains a customers database</li> </ul> </li> </ul> <p>\ud83e\udd73 Congratulations!  Your Azure CosmosDB database is ready!</p>"},{"location":"02%20%7C%20Workshop/05%20%7C%20Setup%20Promptflow/13-local-connections/","title":"13. VSCode Config Connections","text":"<p>[!NOTE] This assumes you completed all Azure resource setup and VS Code configuration for those resources. Now let's setup local Connections so we can run the prompt flow in VS Code later.</p> <ul> <li> <p>[]  01 | Return to the Visual Studio Code editor tab</p> </li> <li> <p>[]  02 | Setup a local third-party backend to store keys</p> <ul> <li>Open the Visual Studio Code terminal </li> <li>Type +++pip install keyrings-alt+++ and hit Enter</li> <li>Installation should complete quickly</li> </ul> </li> <li> <p>[]  03 | Run the notebook to set local prompt flow connections</p> <ul> <li>Locate the \"connections/\" folder</li> <li>Open the create-connections.ipynb Jupyter Notebook.</li> <li>Click Select Kernel, set recommended Python environment </li> <li>Click Clear All Outputs then Run All &amp; verify completion</li> </ul> </li> <li> <p>[]  04 | Validate connections were created</p> <ul> <li>Return to Visual Studio Code terminal</li> <li>Type +++pf connection list+++ and hit Enter</li> <li>Verify 3 connections were created with these names \"contoso-search\", \"contoso-cosmos\", \"aoai-connection\"</li> </ul> </li> </ul> <p>\ud83e\udd73 Congratulations!  Your local connections to the Azure AI project are ready!</p>"},{"location":"02%20%7C%20Workshop/05%20%7C%20Setup%20Promptflow/14-azure-connections/","title":"14. Azure Config Connections","text":"<p>[!NOTE] Your Azure resources are setup. Your local connections are configured. Now let's create cloud connections to run prompt flow in Azure later.</p> <ul> <li>[]  01 | Switch browser tab to +++https://ai.azure.com+++ <ul> <li>Click the \"Build\" option in navbar.</li> <li>Click your AI Project to view its details page.</li> <li>Click the \"Settings\" option</li> <li>Locate the \"Connections\" tab and click \"View All\".</li> </ul> </li> </ul> <p>[!IMPORTANT] Connection names used are critical and must match the names given below. Keep \".env\" open for quick access to required values. Do not copy the enclosing quotes when you copy/paste values from \".env\".</p> <ul> <li> <p>[]  02 | Click New Connection (create +++contoso-search+++) </p> <ul> <li>Service = \"Azure AI Search (Cognitive Services\" </li> <li>Endpoint = from .env \"CONTOSO_SEARCH_SERVICE_ENDPOINT\"</li> <li>API key = from .env \"CONTOSO_SEARCH_KEY\"</li> <li>Connection name = +++contoso-search+++</li> </ul> </li> <li> <p>[]  03 | Click New Connection (create +++aoai-connection+++) </p> <ul> <li>Service = \"Azure OpenAI\" </li> <li>API base = from .env \"CONTOSO_AI_SERVICES_ENDPOINT\"</li> <li>API key = from .env \"CONTOSO_AI_SERVICES_KEY\"</li> <li>Connection name = +++aoai-connection+++</li> </ul> </li> </ul> <p>[!hint] The next step uses Azure Machine Learning Studio instead of Azure AI Studio. We expect this to change with a future product update, allowing you to create all connections from Azure AI Studio.</p> <ul> <li> <p>[]  04 | Creating Custom Connection (+++contoso-cosmos+++) </p> <ul> <li>Visit +++https://ml.azure.com+++ instead</li> <li>Under Recent Workspaces, click project (contoso-chat-aiproj)</li> <li>Select Prompt flow (sidebar), then Connections (tab)</li> <li>Click Create and select Custom from dropdown</li> <li>Name: +++contoso-cosmos+++</li> <li>Provider: Custom (default)</li> <li>Key-value pairs: Add 4 entries (get env var values from .env)<ul> <li>key: +++key+++, value: \"COSMOS_KEY\", check \"is secret\"</li> <li>key: +++endpoint+++ , value: \"COSMOS_ENDPOINT\"</li> <li>key: +++containerId+++, value: +++customers+++</li> <li>key: +++databaseId+++, value: +++contoso-outdoor+++</li> </ul> </li> <li>Click Save to complete step. </li> </ul> </li> <li> <p>[]  06 | Click Refresh on menu bar to validate creation</p> <ul> <li>Verify 3 connections were created with these names \"contoso-search\", \"contoso-cosmos\", \"aoai-connection\"</li> </ul> </li> </ul> <p>\ud83e\udd73 Congratulations!  Your cloud connections to the Azure AI project are ready!</p>"},{"location":"02%20%7C%20Workshop/06%20%7C%20Run%20%26%20Evaluate%20Flow/15-pf-explore/","title":"15. PromptFlow: Codebase","text":"<p>[!NOTE] Our environment, resources and connections are configured. Now, let's learn about prompt flow and how it works. A prompt flow is a DAG (directed acyclic graph) made of up nodes connected together in a flow. Each node is a function tool (written in Python) that can be edited and customized to suit your needs.</p> <ul> <li> <p>[]  01 | Let's explore the Prompt Flow extension</p> <ul> <li>Click the \"Prompt Flow\" icon in the Visual Studio Code sidebar</li> <li>You should see a slide-out menu with the following sections<ul> <li>Quick Access - Create new flows, install dependencies etc,</li> <li>Flows - Lists flows in project (defined by flow.dag.yaml)</li> <li>Tools - Lists available function tools (used in flow nodes)</li> <li>Batch Run History - flows run against data or other runs</li> <li>Connections - Lists connections &amp; helps create them</li> </ul> </li> <li>We'll revisit this later as needed, when executing prompt flows.</li> </ul> </li> <li> <p>[]  02 | Let's understand prompt flow folders &amp; structure</p> <ul> <li>Click the \"Explorer\" icon in the Visual Studio Code sidebar</li> <li>Promptflow can create three kinds of flows:<ul> <li>standard = basic flow folder structure</li> <li>chat = enhances standard flow for conversations</li> <li>evaluation = special flow, assesses outputs of other flows</li> </ul> </li> <li>Explore the \"contoso_chat\" folder for a chat flow:<ul> <li>flow.dag.yaml - defines the flow (inputs, outputs, nodes)</li> <li>source code (.py, .jinja2) - function tools used by flow</li> <li>requirements.txt - defines Python dependencies for flow</li> </ul> </li> <li>Explore the \"eval/\" folder for examples of eval flows<ul> <li>eval/groundedness - tests for single metric (groundedness)</li> <li>eval/multi_flow - tests for multiple metrics (groundedness, fluency, coherance, relevance)</li> <li>eval/evaluate-chat-prompt-flow.ipynb - shows how these are used to evaluate the_contoso_chat_ flow.</li> </ul> </li> </ul> </li> <li> <p>[]  03 | Let's explore a prompt flow in code</p> <ul> <li>Open Visual Studio Code file: contoso-chat/flow.dag.yaml </li> <li>You should see a declarative file with these sections:<ul> <li>environment - requirements.txt to install dependencies</li> <li>inputs - named inputs &amp; properties for flow </li> <li>outputs - named outputs &amp; properties for flow </li> <li>nodes - processing functions (tools) for workflow</li> </ul> </li> </ul> </li> </ul> <p>The \"prompt flow\" is defined by the flow.dag.yaml but the text view does not help us understand the \"flow\" of this process. Thankfully, the Prompt Flow extension gives us a Visual Editor that can help. Let's explore it.</p> <p>\ud83e\udd73 Congratulations!  You're ready to explore a prompt flow visually!</p>"},{"location":"02%20%7C%20Workshop/06%20%7C%20Run%20%26%20Evaluate%20Flow/16-pf-visual/","title":"16. PromptFlow: Visual Editor","text":"<p>[!hint] In the previous section, you should have opened Visual Studio Code, navigated to the contoso-chat folder, and opened the flow.dag.yaml file in the editor pane. We also assume you have the Prompt Flow extension installed correctly (see VS Code extensions sidebar).</p> <ul> <li> <p>[]  01 | View contoso-chat/flow.dag.yaml in the Visual Studio Code editor</p> <ul> <li>Make sure your cursor is at the top of the file in editor panel.</li> <li>You should see a line of menu options similar to the image below. Note: This is an example and not an exact screenshot for this project. It may take a couple of seconds for the menu options to appear so be patient. </li> </ul> </li> <li> <p>[]  02 | Click Visual editor link or use keyboard shortcut: Ctrl + k,v <ul> <li>You should get a split screen view with a visual graph on the right and sectioned forms on the left, as show below. Note: This is an example and not an exact screenshot for this project. </li> <li>Click on any of the \"nodes\" in the graph on the right<ul> <li>The left pane should scroll to the corresponding declarative view, to show the node details.</li> <li>Let's explore our prompt flow components visually, next.</li> </ul> </li> </ul> <li> <p>[]  03 | Explore prompt flow inputs. These start the flow.</p> <ul> <li>customer_id - to look up customer in CosmosDB</li> <li>question - the question that customer is asking</li> <li>chat_history - the conversation history with customer</li> </ul> </li> <li> <p>[]  04 | Explore prompt flow nodes. These are the processing functions.</p> <ul> <li>queston_embedding - use embedding model to embed question text in vector search query</li> <li>retrieve_documents - uses query to retrieve most relevant docs from AI Search index</li> <li>customer_lookup - looks up customer record in parallel, from Azure Cosmos DB database</li> <li>customer_prompt - populate customer prompt \"template\" with customer &amp; search results</li> <li>llm_response - uses chat completion model to generate a response to customer query using this enhanced prompt</li> </ul> </li> <li>[]  04 | Explore prompt flow outputs. These end the flow.<ul> <li>Returns the LLM-generated response to the customer</li> </ul> </li> <p>This defines the processing pipeline for your LLM application from user input, to returned response. To execute the flow, we need a valid Python runtime. We can use the default runtime available to use in GitHub Codespaces to run this from Visual Studio Code. Let's do that next.</p> <p>\ud83e\udd73 Congratulations!  You're ready to run your Prompt flow.</p>"},{"location":"02%20%7C%20Workshop/06%20%7C%20Run%20%26%20Evaluate%20Flow/17-pf-run/","title":"17. PromptFlow: Run","text":"<p>[!hint] In the previous section, you should have opened Visual Studio Code, navigated to the contoso-chat folder, and opened the flow.dag.yaml file in the Visual Editor view.</p> <ul> <li> <p>[]  01 | View contoso-chat/flow.dag.yaml in the Visual Studio Code editor</p> <ul> <li>Make sure your cursor is at the top of the file in editor panel.</li> <li>Make sure you are in the Visual editor with a view like this. Note: this is an example screenshot, and not the exact one for this lab.     </li> </ul> </li> <li> <p>[]  02 | Run the prompt flow locally</p> <ul> <li>Tip: Keep VS Code terminal open to view console output. </li> <li>Look at the 2nd line (starting with \"+LLM\") in Visual Editor.</li> <li>Look for a 'tool' icon at right: It should show a valid Python env.</li> <li>Look for a 'play' icon next to it: The tooltip should read \"Run All\".</li> <li>Click Run All (or use \"Shift + F5\" on keyboard)</li> <li>Select \"Run it with standard mode\" in dropdown</li> </ul> </li> <li> <p>[]  03 | Explore inputs and outputs of flow</p> <ul> <li>The Inputs section will have these values:<ul> <li>chat_history: prior turns in chat (default=blank)</li> <li>question: customer's most recent question</li> <li>customerid: to help look up relevant customer context (e.g., order history) to refine response with more relevant context. This is the basis for RAG (retrieval-augmented generation).</li> </ul> </li> <li>The Contoso Outdoors web app provides these inputs (in demo)</li> <li>Note the contents of the Flow run outputs tab under \"Outputs\" section<ul> <li>Use the navigation tools to show the complete answer: Is it a good answer to the input question?</li> <li>Use the nvaigation tools to explore the returned context (products relevant to the customer's question). _Are these good selections for context?</li> </ul> </li> </ul> </li> <li> <p>[]  04 | Explore Individual Flow Nodes</p> <ul> <li>Observe node status colors in VS Code (green=success, red=error)</li> <li>Click any node. The left pane will scroll to show execution details.</li> <li>Click the Code: link in component to see function executed here.</li> </ul> </li> <li> <p>[]  05 | Explore Run Stats &amp; Traces</p> <ul> <li>Click the \"Terminal\" tab. It should show final response returned. </li> <li>Click the \"Prompt Flow\" tab. Select a node in visual editor.<ul> <li>Tab shows \"node name, Tokens, duration\" stats for node.</li> <li>Click the line in table. You should see more details in pane.</li> </ul> </li> </ul> </li> <li> <p>[]  06 | Try a new input</p> <ul> <li>In Inputs, change question to \"What is a good tent for a beginner?\"</li> <li>Click Run All, explore outputs as before.</li> <li>In Inputs, change customerId (to value between 1 and 12)</li> <li>Click Run All, compare this output to before.</li> <li>Experiment with other input values and analyze outputs.</li> </ul> </li> </ul> <p>\ud83e\udd73 Congratulations!  You ran your contoso-chat prompt flow successfully in the local runtime on GitHub Codespaces. Next, we want to evaluate the performance of the flow.</p>"},{"location":"02%20%7C%20Workshop/06%20%7C%20Run%20%26%20Evaluate%20Flow/18-pf-evaluate/","title":"18. PromptFlow: Evaluate","text":"<p>[!NOTE] You've built and run the contoso-chat prompt flow locally using the Visual Studio Code Prompt Flow extension and SDK. Now it's time to evaluate the quality of your LLM app response to see if it's performing up to expectations. Let's dive in.</p> <ul> <li> <p>[]  01 | First, run evaluate-chat-prompt-flow.ipynb</p> <ul> <li>Locate the \"eval/\" folder</li> <li>Open evaluate-chat-prompt0flow.ipynb.</li> <li>Click Select Kernel, use default Python env</li> <li>Click Clear All Outputs, then Run All </li> <li>Execution takes some time. Until then ....</li> </ul> </li> <li> <p>[]  02 | Let's explore what the notebook does</p> <ul> <li>Keep Jupyter notebook open \"Editor\" pane</li> <li>Open VS Code \"Explorer\" pane, select Outline</li> <li>You should see these code sections:<ul> <li>Local Evaluation - Groundedness</li> <li>Local Evaluation - Multiple Metrics</li> <li>AI Studio Azure - Batch run, json dataset</li> <li>Cloud Evaluation - Multi-flow, json dataset</li> </ul> </li> <li>Let's understand what each does.</li> </ul> </li> <li> <p>[]  03 | Local Evalution : Explore Groundedness</p> <ul> <li>Evaluates contoso-chat for groundedness </li> <li>This measures how well the model's generated answers align with information from the source data (user-defined context).</li> <li>Example: We test if the answer to the question \"Can you tell me about your jackets\" is grounded in the product data we indexed previously. </li> </ul> </li> <li> <p>[]  04 | Local Evaluation : Explore Multiple Metrics</p> <ul> <li>Evaluates contoso-chat using 4 key metrics:</li> <li>Groundedness = How well does model's generated answers align with information from the source (product) data?</li> <li>Relevance = Extent to which the model's generated responses are pertinent and directly related to the given questions.</li> <li>Coherence = Ability to generate text that reads naturally, flows smoothly, and resembles human-like language in responses.</li> <li>Fluency = Measures the grammatical proficiency of a generative AI's predicted answer.</li> </ul> </li> </ul> <p>[!NOTE] The above evaluation tests ran against a single test question. For more comprehensive testing, we can use Azure AI Studio to run batch tests using the same evaluation prompt, with the data/salestestdata.jsonl dataset. Let's understand how that works.</p> <ul> <li> <p>[]  05 | Base Run : With evaluation JSON dataset</p> <ul> <li>Use Azure AI Studio with automatic runtime</li> <li>Use \"data/salestestdata.jsonl\" as batch test data</li> <li>Do a base-run using contoso-chat prompt flow</li> <li>View results in notebook - visualized as table</li> </ul> </li> <li> <p>[]  06 | Eval Run : With evaluation JSON dataset</p> <ul> <li>Use Azure AI Studio with automatic runtime</li> <li>Use \"data/salestestdata.jsonl\" as batch test data</li> <li>Do multi-flow eval with base run as variant</li> <li>View results in notebook - visualized as table</li> <li>Results should now show the 4 eval metrics</li> <li>Optionally, click Web View link in final output<ul> <li>See Visualize outputs (Azure AI Studio)</li> <li>Learn more on viewing eval results</li> </ul> </li> </ul> </li> <li> <p>[]  07 | Review Evaluation Output</p> <ul> <li>Check the Jupyter Notebook outputs</li> <li>Verify execution run completed successfully</li> <li>Review evaluation metrics to gain insight  </li> </ul> </li> </ul> <p>\ud83e\udd73 Congratulations!  You've evaluated your contoso-chat flow for single-data and batch data runs, using single-metric and multi-metric eval flows. Now you're ready to deploy the flow so apps can use it. </p>"},{"location":"02%20%7C%20Workshop/07%20%7C%20Deploy%20%26%20Use%20Flow/19-pf-deploy/","title":"19. PromptFlow: Deploy","text":"<p>[!hint] Till now, you've explored, built, tested, and evaluated, the prompt flow from Visual Studio Code, as a developer. Now it's time to deploy the flow to production so applications can use the endpoint to make requests and receive responses in real time.</p> <p>Deployment Options: We will be using Azure AI Studio to deploy our prompt flow from a UI. You can also deploy the flow programmatically using the Azure AI Python SDK. </p> <p>Deployment Process: We'll discuss the 4 main steps: - First, upload the prompt flow to Azure AI Studio - Next, test upload then deploy it interactively - Finally, use deployed endpoint (from built-in test) - Optionally: use deployed endpoint (from real app)</p> <p>[!note] 1: Upload Prompt Flow to Azure AI Studio. </p> <ul> <li> <p>[]  01 | Return to the Visual Studio Code editor tab</p> <ul> <li>Locate the \"deployment/\" folder</li> <li>Open push_and_deploy_pf.ipynb.</li> <li>Click Select Kernel, use default Python env</li> <li>Click Clear All Outputs, then Run All </li> <li>This should complete in just a few minutes.</li> </ul> </li> <li> <p>[]  02 | Verify Prompt Flow was created</p> <ul> <li>Click the flow_portal_url link in output</li> <li>It should open Azure AI Studio to flow page</li> <li>Verify that the visual DAG is for contoso-chat</li> </ul> </li> <li> <p>[]  03 | Setup Automated Runtime in Azure</p> <ul> <li>Click Select runtime dropdown </li> <li>Select Automatic Runtime, click Start</li> <li>Takes a few mins, watch progress indicator.chat</li> </ul> </li> <li> <p>[]  04 | Run Prompt Flow in Azure</p> <ul> <li>On completion, you should see a \u2705</li> <li>Now click the blue Run button</li> <li>Run should complete in a few minutes.</li> <li>Verify that all graph nodes are green (success)</li> </ul> </li> </ul> <p>[!note] 2: Deploy Prompt Flow now that it's tested</p> <ul> <li> <p>[]  01 | Click the Deploy option in flow page</p> <ul> <li>Opens a Deploy wizard flow</li> <li>Endpoint name: use +++contoso-chat-aiproj-ep+++</li> <li>Deployment name: use +++contoso-chat-aiproj-deploy+++</li> <li>Keep defaults, click Review+Create.</li> <li>Review configuration, click Create.</li> </ul> </li> <li> <p>[]  02-A | Check Deployment status (option A)</p> <ul> <li>Navigate to +++https://ai.azure.com+++</li> <li>Click Build &gt; Your AI Project (contoso-chat-aiproj)</li> <li>Click Deployments and hit Refresh</li> <li>You should see \"Endpoint\" listing with Updating</li> <li>Refresh periodically till it shows Succeeded</li> </ul> </li> <li> <p>[]  02-B | Check Deployment status (option B)</p> <ul> <li>Navigate to +++https://ml.azure.com+++</li> <li>Click the notifications icon (bell) in navbar</li> <li>This should slide out a list of status items</li> <li>Watch for all pending tasks to go green.</li> </ul> </li> </ul> <p>[!alert] The deployment process can take 10 minutes or more. Use the time to explore other things.</p> <ul> <li> <p>[]  03 | Deployment succeeded</p> <ul> <li>Go back to the Deployments list in step 02-A</li> <li>Click your deployment to view details page.</li> <li>Wait till page loads and menu items update</li> <li>You should see a menu with these items<ul> <li>Details - status &amp; endpoint info</li> <li>Consume - code samples, URL &amp; keys</li> <li>Test - interactive testing UI</li> <li>Monitoring and Logs - for LLMOps</li> </ul> </li> </ul> </li> <li> <p>[]  04 | Consume Deployment</p> <ul> <li>Click the Consume tab</li> <li>You should see <ul> <li>the REST URL for endpoint</li> <li>the authentication keys for endpoint</li> <li>code snippets for key languages</li> </ul> </li> <li>Use this if testing from an app. In the next step, we'll explore using a built-in test instead.</li> </ul> </li> </ul> <p>[!note] 1: Use Deployed Endpoint with a built-in test. </p> <ul> <li>[]  01 | Click the Test option in deployment page<ul> <li>Enter \"[]\" for chat_history</li> <li>Enter +++What can you tell me about your jackets?+++ for question</li> <li>Click Test and watch Test result pane</li> <li>Test result output should show LLM app response</li> </ul> </li> </ul> <p>Explore this with other questions or by using different customer Id or chat_history values if time permits.</p> <p>\ud83e\udd73 Congratulations!  You made it!! You just setup, built, ran, evaluated, and deployed a RAG-based LLM application using Azure AI Studio and Prompt Flow.</p>"},{"location":"02%20%7C%20Workshop/07%20%7C%20Deploy%20%26%20Use%20Flow/20-lab-recap/","title":"Lab Recap","text":"<p>[!hint] What We Learned Today</p> <p>We started with a simple goal: Build an LLM-based chat app that used Retrieval Augmented Generation (RAG) to answer questions relevant to a product catalog.</p> <p>We learned about LLM Ops: Specifically, we identified a number of steps that need to be chained together in a workflow, to build, deploy &amp; use performant LLM Apps.</p> <p>We learned about Azure AI Studio: Specifically, we learned how to provision an Azure AI project using an Azure AI resource with selected Model deployments. We learned to build a RAG solution with Azure AI Search and Azure Cosmos DB. And we learned to upload, deploy, run, and test, prompt flows in Azure.</p> <p>We learned about Prompt Flow: Specifically, we learned how to create, evalute, test, and deploy, a prompt flow using a VS Code extension to streamline end-to-end development for an LLM-based app. And we learned how to upload the flow to Azure AI Studio, and replicate the steps completely in the cloud.</p> <p>Along the way, we learned what LLM Ops is and why having these tools to simplify and orchestrate end-to-end development workflows is critical for building the next generation of Generative AI applications at cloud scale.</p> <p>[!hint] What We Can Try Next</p> <ul> <li>Explore Next Steps for LLMOpss. <ul> <li>Add GitHub Actions, Explore Intents </li> <li>See README: +++https://github.com/Azure-Samples/contoso-chat+++ README </li> </ul> </li> <li>Explore Usage in Real Application. <ul> <li>Integrate &amp; use deployed endpoint in web app</li> <li>See README: +++https://github.com/Azure-Samples/contoso-web+++</li> </ul> </li> </ul> <p>[!hint] Where Can You Learn More?</p> <ul> <li>Explore Developer Resources: Azure AI Developer Hub</li> <li>Join The Community: Azure AI Discord</li> </ul> <p>\ud83c\udfc6 | THANK YOU FOR JOINING US!</p>"},{"location":"02%20%7C%20Workshop/08%20%7C%20Bonus%20Exercises/1-App-Integration/","title":"1 | Contoso Website Chat","text":""},{"location":"02%20%7C%20Workshop/08%20%7C%20Bonus%20Exercises/2-Actions-Integration/","title":"2 | GitHub Action Deploy","text":""},{"location":"02%20%7C%20Workshop/08%20%7C%20Bonus%20Exercises/3-Intents-Integration/","title":"3 | Intent-Based Routing","text":""},{"location":"02%20%7C%20Workshop/08%20%7C%20Bonus%20Exercises/4-Content-Filtering/","title":"4 | Content Filtering","text":""},{"location":"02%20%7C%20Workshop/09%20%7C%20Troubleshooting/","title":"1 | Debug Issues","text":"<p>Don't see your issue listed?</p> <p>Submit an issue to the repository with the following information, to help us debug and update the list</p> <ul> <li>Which workshop section were you on? (e.g., Setup Promptflow)</li> <li>What step of that section were you on? </li> <li>What was the error message or behavior you saw?</li> <li>What was the behavior you expected?</li> <li>Screenshot of the issue (if relevant).</li> </ul> <p>When submitting any issue please make sure you mask any secrets or personal information (e.g., your Azure subscription id) to avoid exposing that information publicly.</p> <p>This page lists any frequently-encountered issues for this workshop, with some suggestions on how to debug these on your own. Note that Azure AI Studio (Preview) and Promptflow are both evolving rapidly - some known issues may be resolved in future updates to those tools.</p>"},{"location":"02%20%7C%20Workshop/09%20%7C%20Troubleshooting/#1-model-deployments","title":"1. Model Deployments","text":"<p>1.1 | Requested Model Not Found</p> <ul> <li>What Happened:<ul> <li>Required model does not show in options for Create</li> </ul> </li> <li>Possible Causes:<ul> <li>Model not available in region where the Azure AI resource is provisioned</li> <li>Model available but no quota left in subscription</li> </ul> </li> <li>Debug Suggestions:<ul> <li>Open \"Manage\" page under Azure AI Studio</li> <li>Pick \"Quota\" tab, select your subscription</li> <li>Check quota for region Azure AI resource is in</li> <li>If none left, look for region that still has quota</li> <li>Switch to that region and try to create deployment.</li> </ul> </li> </ul>"},{"location":"03%20%7C%20Concepts/01-RAG/","title":"01 | The RAG Pattern","text":"<p>This page is under construction. Please check back!</p> <p></p>"},{"location":"03%20%7C%20Concepts/02-Metrics/","title":"02 | The Evaluation Metrics","text":"<p>This page is under construction. Please check back!</p> <p></p>"},{"location":"04%20%7C%20Tooling/01-Dev-Containers/","title":"01 | Dev Containers","text":"<p>This page is under construction. Please check back!</p> <p></p>"},{"location":"04%20%7C%20Tooling/02-Visual-Studio-Code/","title":"02 | Visual Studio Code","text":"<p>This page is under construction. Please check back!</p> <p></p>"},{"location":"04%20%7C%20Tooling/03-Prompt-Flow/","title":"03 | Prompt Flow","text":"<p>This page is under construction. Please check back!</p> <p></p>"},{"location":"04%20%7C%20Tooling/04-Azure-AI-Studio/","title":"04 | Azure AI Studio","text":"<p>This page is under construction. Please check back!</p> <p></p>"}]}